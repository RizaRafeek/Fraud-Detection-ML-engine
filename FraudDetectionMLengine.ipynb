{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYY18ojUllyo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# 1. Paste your token between the quotes below\n",
        "os.environ['KAGGLE_USERNAME'] = \"enter your kaggle username url\" # Look at your Kaggle profile URL to find this\n",
        "os.environ['KAGGLE_KEY'] = \"enter your kaggle key\"\n",
        "\n",
        "print(\"Credentials set!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a folder for the data\n",
        "!mkdir -p data\n",
        "\n",
        "# Download the specific finance dataset\n",
        "!kaggle datasets download -d aryan208/financial-transactions-dataset-for-fraud-detection\n",
        "\n",
        "# Unzip it\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"financial-transactions-dataset-for-fraud-detection.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"data\")\n",
        "\n",
        "print(\"Dataset is ready in the 'data' folder!\")"
      ],
      "metadata": {
        "id": "hDdjq9wvmq3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Let's see what is actually inside the 'data' folder\n",
        "files_in_data = os.listdir('data')\n",
        "print(f\"Files found in the data folder: {files_in_data}\")\n",
        "\n",
        "# 2. Automatically pick the first CSV file found\n",
        "csv_file = [f for f in files_in_data if f.endswith('.csv')][0]\n",
        "file_path = os.path.join('data', csv_file)\n",
        "\n",
        "# 3. Load it\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(f\"\\nSuccess! Loaded file: {csv_file}\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ztmgz6LfnKEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the fuzzy matching tool\n",
        "!pip install thefuzz"
      ],
      "metadata": {
        "id": "bMcFysVlolen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the unique merchant names to see the mess\n",
        "unique_merchants = df['Merchant'].unique()\n",
        "print(f\"Total unique merchant names found: {len(unique_merchants)}\")\n",
        "print(\"First 10 merchant names in your data:\")\n",
        "print(unique_merchants[:10])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sVpGZd5fpIc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This lists all the actual column names in your table\n",
        "print(\"The columns in this dataset are:\")\n",
        "print(df.columns.tolist())"
      ],
      "metadata": {
        "id": "o3tR6IDUpow4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N8oib51CpuiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Group the data by category and sum the amounts\n",
        "category_spend = df.groupby('merchant_category')['amount'].sum().sort_values(ascending=False)\n",
        "\n",
        "# 2. Create a bar chart\n",
        "plt.figure(figsize=(12,6))\n",
        "category_spend.plot(kind='bar', color='skyblue')\n",
        "plt.title('Total Spending by Merchant Category')\n",
        "plt.ylabel('Total Amount ($)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "print(\"This chart shows us where the bulk of the money is going!\")"
      ],
      "metadata": {
        "id": "I6GYtCRCqrQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Calculate the mean and standard deviation for the 'amount' column\n",
        "mean_amt = df['amount'].mean()\n",
        "std_amt = df['amount'].std()\n",
        "\n",
        "# 2. Apply the Z-Score formula: (Value - Mean) / Standard Deviation\n",
        "df['z_score'] = (df['amount'] - mean_amt) / std_amt\n",
        "\n",
        "# 3. Flag anything with a Z-Score greater than 3 as an Anomaly\n",
        "df['is_anomaly'] = df['z_score'].abs() > 3\n",
        "\n",
        "# 4. Show us only the anomalies we found!\n",
        "anomalies = df[df['is_anomaly'] == True]\n",
        "print(f\"Total Anomalies Found: {len(anomalies)}\")\n",
        "anomalies[['merchant_category', 'amount', 'z_score']].head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6gC-wuFrr04m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Calculate Z-Score specifically for each category\n",
        "df['z_score_category'] = df.groupby('merchant_category')['amount'].transform(\n",
        "    lambda x: (x - x.mean()) / x.std()\n",
        ")\n",
        "\n",
        "# 2. Flag anomalies again with the new specific score\n",
        "df['is_anomaly_v2'] = df['z_score_category'].abs() > 3\n",
        "\n",
        "# 3. Check the count again\n",
        "refined_anomalies = df[df['is_anomaly_v2'] == True]\n",
        "print(f\"Total Refined Anomalies Found: {len(refined_anomalies)}\")\n",
        "\n",
        "# 4. Let's see the most 'extreme' ones\n",
        "refined_anomalies[['merchant_category', 'amount', 'z_score_category']].sort_values(by='z_score_category', ascending=False).head()"
      ],
      "metadata": {
        "id": "eNy7qtY6zRRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Let's look at one category to see why the math is flagging so much\n",
        "sample_cat = df['merchant_category'].unique()[0]\n",
        "subset = df[df['merchant_category'] == sample_cat]\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.histplot(subset['amount'], bins=50, kde=True, color='purple')\n",
        "plt.title(f'Distribution of Amounts for {sample_cat}')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Average amount for {sample_cat}: {subset['amount'].mean()}\")\n",
        "print(f\"Wiggle room (Std Dev) for {sample_cat}: {subset['amount'].std()}\")"
      ],
      "metadata": {
        "id": "5JeKD9qxzmmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define a function to find outliers using the IQR method\n",
        "def find_iqr_outliers(group):\n",
        "    Q1 = group.quantile(0.25)\n",
        "    Q3 = group.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return (group > upper_bound)\n",
        "\n",
        "# 2. Apply this logic to every category\n",
        "df['is_outlier_iqr'] = df.groupby('merchant_category')['amount'].transform(find_iqr_outliers)\n",
        "\n",
        "# 3. Count the new, smarter anomalies\n",
        "iqr_anomalies = df[df['is_outlier_iqr'] == True]\n",
        "print(f\"Total Robust Anomalies Found: {len(iqr_anomalies)}\")\n",
        "\n",
        "# 4. Look at the biggest ones\n",
        "iqr_anomalies[['merchant_category', 'amount', 'date']].sort_values(by='amount', ascending=False).head()"
      ],
      "metadata": {
        "id": "-HUgBi640K8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Fix the KeyError: Make sure 'date' exists\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "df['date'] = df['timestamp'].dt.date\n",
        "\n",
        "# 2. Define our \"Human + Math\" Filter\n",
        "# We want items that the IQR flagged AND are above a certain dollar amount (e.g., 500)\n",
        "# This removes the \"pennies\" that the math flagged as anomalies.\n",
        "hard_threshold = 500\n",
        "\n",
        "final_anomalies = df[\n",
        "    (df['is_outlier_iqr'] == True) &\n",
        "    (df['amount'] > hard_threshold)\n",
        "]\n",
        "\n",
        "print(f\"Final Professional Anomalies: {len(final_anomalies)}\")\n",
        "\n",
        "# 3. View the results (Now including 'date' safely!)\n",
        "final_anomalies[['date', 'merchant_category', 'amount']].sort_values(by='amount', ascending=False).head(10)"
      ],
      "metadata": {
        "id": "6EkBz35I0jFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Fix the ValueError: Use 'ISO8601' to handle different timestamp lengths\n",
        "# We also add errors='coerce' just in case there's a totally broken piece of text\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'], format='ISO8601', errors='coerce')\n",
        "\n",
        "# Create the date column (dropping any rows that couldn't be converted)\n",
        "df = df.dropna(subset=['timestamp'])\n",
        "df['date'] = df['timestamp'].dt.date\n",
        "\n",
        "# 2. Define our \"Human + Math\" Filter\n",
        "# We only want outliers that are actually significant amounts\n",
        "hard_threshold = 500\n",
        "\n",
        "final_anomalies = df[\n",
        "    (df['is_outlier_iqr'] == True) &\n",
        "    (df['amount'] > hard_threshold)\n",
        "]\n",
        "\n",
        "print(f\"Final Professional Anomalies: {len(final_anomalies)}\")\n",
        "\n",
        "# 3. View the results\n",
        "if not final_anomalies.empty:\n",
        "    print(final_anomalies[['date', 'merchant_category', 'amount']].sort_values(by='amount', ascending=False).head(10))\n",
        "else:\n",
        "    print(\"No anomalies found over $500. Try lowering the hard_threshold to 100.\")"
      ],
      "metadata": {
        "id": "ewvlMYN-07CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Count total transactions per category\n",
        "total_counts = df['merchant_category'].value_counts()\n",
        "\n",
        "# 2. Count anomaly transactions per category\n",
        "anomaly_counts = final_anomalies['merchant_category'].value_counts()\n",
        "\n",
        "# 3. Combine them into a \"Risk Table\"\n",
        "risk_report = pd.DataFrame({\n",
        "    'Total_Transactions': total_counts,\n",
        "    'Total_Anomalies': anomaly_counts\n",
        "}).fillna(0) # Replace missing values with 0\n",
        "\n",
        "# 4. Calculate the Risk Percentage (Density)\n",
        "risk_report['Risk_Percentage'] = (risk_report['Total_Anomalies'] / risk_report['Total_Transactions']) * 100\n",
        "\n",
        "# 5. Sort by highest risk\n",
        "risk_report = risk_report.sort_values(by='Risk_Percentage', ascending=False)\n",
        "\n",
        "print(\"--- MERCHANT RISK REPORT ---\")\n",
        "print(risk_report)"
      ],
      "metadata": {
        "id": "Pvy-w_CY2-k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the 'Fence' for each category\n",
        "stats = df.groupby('merchant_category')['amount'].agg([\n",
        "    lambda x: x.quantile(0.25),\n",
        "    lambda x: x.quantile(0.75)\n",
        "]).rename(columns={'<lambda_0>': 'Q1', '<lambda_1>': 'Q3'})\n",
        "\n",
        "# Calculate IQR and the Upper Fence\n",
        "stats['IQR'] = stats['Q3'] - stats['Q1']\n",
        "stats['Upper_Fence'] = stats['Q3'] + (1.5 * stats['IQR'])\n",
        "\n",
        "print(\"--- THE ANOMALY FENCES ---\")\n",
        "print(stats[['Q1', 'Q3', 'Upper_Fence']].sort_values(by='Upper_Fence', ascending=False))"
      ],
      "metadata": {
        "id": "oZWdxrKh6Sq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set the style\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# Create a Boxplot of the top 5 most common categories to keep it clean\n",
        "top_categories = df['merchant_category'].value_counts().nlargest(5).index\n",
        "df_plot = df[df['merchant_category'].isin(top_categories)]\n",
        "\n",
        "# Plotting\n",
        "sns.boxplot(x='merchant_category', y='amount', data=df_plot, palette=\"Set2\")\n",
        "\n",
        "# Setting labels\n",
        "plt.title('Spending Distribution & Anomaly Fences by Category', fontsize=16)\n",
        "plt.ylabel('Transaction Amount ($)')\n",
        "plt.xlabel('Merchant Category')\n",
        "plt.yscale('log') # Using log scale because your data has a 'Long Tail'\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gZI48LywDqJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 'FacetGrid' to see each category clearly on its own scale\n",
        "g = sns.FacetGrid(df_plot, col=\"merchant_category\", sharey=False, height=5, aspect=0.8)\n",
        "g.map(sns.boxplot, \"amount\", order=None, palette=\"Set2\")\n",
        "\n",
        "plt.subplots_adjust(top=0.8)\n",
        "g.fig.suptitle('Zoomed-In View: Individual Category Distributions', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S5PtxIHGGIap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the high-value anomaly list\n",
        "final_report = final_anomalies[['date', 'merchant_category', 'amount']].sort_values(by='amount', ascending=False)\n",
        "\n",
        "# Save to a CSV file you can download\n",
        "final_report.to_csv('High_Value_Anomalies.csv', index=False)\n",
        "\n",
        "print(f\"Success! We found {len(final_report)} professional anomalies.\")\n",
        "print(\"The file 'High_Value_Anomalies.csv' is ready to download from your Colab files tab.\")"
      ],
      "metadata": {
        "id": "5CRMHy-xFPOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instead of user_id, let's see which CATEGORY is the 'leakies'\n",
        "watchlist = final_anomalies['merchant_category'].value_counts()\n",
        "\n",
        "print(\"--- TOP RISK CATEGORIES ---\")\n",
        "print(watchlist)\n",
        "\n",
        "# Let's calculate the average 'Damage' ($) per category\n",
        "damage_report = final_anomalies.groupby('merchant_category')['amount'].sum().sort_values(ascending=False)\n",
        "print(\"\\n--- TOTAL DOLLAR RISK PER CATEGORY ---\")\n",
        "print(damage_report)"
      ],
      "metadata": {
        "id": "w8X_snGRIodG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Z-Score feature so the AI knows 'relative' risk\n",
        "df['z_score_category'] = df.groupby('merchant_category')['amount'].transform(\n",
        "    lambda x: (x - x.mean()) / x.std()\n",
        ").fillna(0) # Fill empty spots so the AI doesn't crash"
      ],
      "metadata": {
        "id": "CeV80JIvJJS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# 1. Select the signals: Raw Amount and the Z-Score Context\n",
        "X = df[['amount', 'z_score_category']]\n",
        "\n",
        "# 2. Initialize the Forest\n",
        "# We use 100 trees (n_estimators) to vote on what is 'weird'\n",
        "iso_forest = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\n",
        "\n",
        "# 3. Fit and Predict\n",
        "# It learns the patterns and labels them: 1 (Normal) or -1 (Anomaly)\n",
        "df['ml_anomaly_flag'] = iso_forest.fit_predict(X)\n",
        "\n",
        "print(\"ML Training Complete.\")"
      ],
      "metadata": {
        "id": "PPV3c6m2JnHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out the AI-detected anomalies\n",
        "ml_anomalies = df[df['ml_anomaly_flag'] == -1]\n",
        "\n",
        "# Re-calculate your old statistical anomalies for comparison\n",
        "stat_anomalies = df[df['z_score_category'].abs() > 3]\n",
        "\n",
        "print(f\"--- RESULTS ---\")\n",
        "print(f\"Statistical Method (Z-Score > 3) found: {len(stat_anomalies)}\")\n",
        "print(f\"AI Method (Isolation Forest) found: {len(ml_anomalies)}\")"
      ],
      "metadata": {
        "id": "CXDxUouzJ7nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Turn the 'timestamp' column into real dates/times\n",
        "df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
        "\n",
        "# 2. Sort by User (cc_num) and Time so we can see their 'trail'\n",
        "df = df.sort_values(by=['cc_num', 'trans_date_trans_time'])\n",
        "\n",
        "# 3. THE MAGIC: Count how many times each card (cc_num) appeared in the last 24 hours\n",
        "# We use a 'rolling' window of 24 hours ('24H')\n",
        "df['trans_velocity_24h'] = df.groupby('cc_num').rolling('24H', on='trans_date_trans_time')['amount'].count().values\n",
        "\n",
        "print(\"Velocity feature created! Now the AI can see 'Frequency'.\")"
      ],
      "metadata": {
        "id": "p6Y7mcM6OJoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "id": "0VcXbqpdOnmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Convert the 'timestamp' column to a real clock\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "\n",
        "# 2. Sort so the 'story' of each account is in order\n",
        "df = df.sort_values(by=['sender_account', 'timestamp'])\n",
        "\n",
        "# 3. Calculate your own Rolling 24H Velocity\n",
        "# We count how many times 'sender_account' appears in a 24-hour sliding window\n",
        "df['my_custom_velocity'] = df.groupby('sender_account').rolling('24H', on='timestamp')['amount'].count().values\n",
        "\n",
        "print(\"New Feature 'my_custom_velocity' is ready.\")"
      ],
      "metadata": {
        "id": "I3OoEuuHPGbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Convert to datetime but DON'T crash on errors\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "\n",
        "# 2. Drop rows where the timestamp is now missing (NaT)\n",
        "# If we don't do this, the sorting and rolling count will fail\n",
        "df = df.dropna(subset=['timestamp'])\n",
        "\n",
        "# 3. Now Sort (Using 'sender_account' as the ID)\n",
        "df = df.sort_values(by=['sender_account', 'timestamp'])\n",
        "\n",
        "# 4. Re-run the Rolling Velocity\n",
        "df['my_custom_velocity'] = df.groupby('sender_account').rolling('24H', on='timestamp')['amount'].count().values\n",
        "\n",
        "print(f\"Data cleaned. {len(df)} rows remaining. Velocity feature is LIVE.\")"
      ],
      "metadata": {
        "id": "5ORq79NmPai8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Look for anything faster than a commercial jet (e.g., 1000 km/h)\n",
        "impossible_hits = df[df['travel_speed_kmh'] > 1000]\n",
        "\n",
        "# 2. Show the evidence\n",
        "print(f\"Detected {len(impossible_hits)} impossible travel events.\")\n",
        "if len(impossible_hits) > 0:\n",
        "    print(impossible_hits[['sender_account', 'timestamp', 'dist_km', 'time_diff_hours', 'travel_speed_kmh']].head(10))"
      ],
      "metadata": {
        "id": "vQN0t-FSPK6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "97v-zoBjUKKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 2: The Unified Production Pipeline\n",
        "---\n",
        "### **Project Milestone**\n",
        "This section combines all previous research (Z-Scores, Velocity, and Geo-Speed) into a single, high-performance **Isolation Forest** model."
      ],
      "metadata": {
        "id": "fDnHVeWnUbbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# --- STEP 1: DATA LOADING ---\n",
        "files_in_data = os.listdir('data')\n",
        "csv_file = [f for f in files_in_data if f.endswith('.csv')][0]\n",
        "df = pd.read_csv(os.path.join('data', csv_file))\n",
        "\n",
        "# --- STEP 2: DATA CLEANING ---\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "df = df.dropna(subset=['timestamp'])\n",
        "df = df.sort_values(by=['sender_account', 'timestamp']).reset_index(drop=True)\n",
        "\n",
        "# --- STEP 3: FEATURE ENGINEERING ---\n",
        "# Category Context (Z-Score)\n",
        "df['z_score_category'] = df.groupby('merchant_category')['amount'].transform(\n",
        "    lambda x: (x - x.mean()) / x.std()\n",
        ").fillna(0)\n",
        "\n",
        "# Rolling Velocity (24h count)\n",
        "df['my_custom_velocity'] = df.groupby('sender_account').rolling('24h', on='timestamp')['amount'].count().values\n",
        "\n",
        "# --- STEP 4: THE AI MODEL ---\n",
        "# Updated features based on your actual dataset columns\n",
        "features = [\n",
        "    'amount',\n",
        "    'z_score_category',\n",
        "    'my_custom_velocity',\n",
        "    'geo_anomaly_score',\n",
        "    'time_since_last_transaction'\n",
        "]\n",
        "\n",
        "X = df[features].fillna(0)\n",
        "\n",
        "# Initialize and Predict\n",
        "iso_forest = IsolationForest(n_estimators=100, contamination=0.02, random_state=42)\n",
        "df['ml_anomaly_flag'] = iso_forest.fit_predict(X)\n",
        "\n",
        "# --- STEP 5: RESULTS (FIXED KEYERROR) ---\n",
        "ml_anomalies = df[df['ml_anomaly_flag'] == -1]\n",
        "\n",
        "print(f\"âœ… Analysis Complete.\")\n",
        "print(f\"Total Transactions: {len(df)}\")\n",
        "print(f\"Fraudulent Patterns Detected: {len(ml_anomalies)}\")\n",
        "\n",
        "if not ml_anomalies.empty:\n",
        "    print(\"\\n--- DETECTED ANOMALIES (Top 5) ---\")\n",
        "    # We only list columns that WE KNOW exist in your list\n",
        "    display_cols = ['merchant_category', 'amount', 'geo_anomaly_score', 'my_custom_velocity']\n",
        "    print(ml_anomalies[display_cols].head())"
      ],
      "metadata": {
        "id": "A97DIy2rZCxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Project Conclusion**\n",
        "The final model successfully isolated 100,000 anomalies from a dataset of 5 million transactions using a 2% contamination threshold.\n",
        "\n",
        "**Model Strength:** By combining `amount` with `geo_anomaly_score` and my custom `24h_velocity`, the system identifies complex fraud like:\n",
        "1. **High-Value Spikes:** Large transactions that deviate from category norms.\n",
        "2. **Fast-Following Hits:** Multiple transactions from the same account in a 24-hour window.\n",
        "3. **Location Mismatches:** Transactions occurring in geographically improbable sequences."
      ],
      "metadata": {
        "id": "xZopWFReaTTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 3: Visualizing the Invisible\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IjGWbNogP8fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 6: DATA SAMPLING FOR VISUALIZATION ---\n",
        "# 1. Take all the anomalies (the rare \"needles\")\n",
        "anomalies_sample = df[df['ml_anomaly_flag'] == -1]\n",
        "\n",
        "# 2. Take a small random sample of normal transactions (the \"haystack\")\n",
        "normal_sample = df[df['ml_anomaly_flag'] == 1].sample(n=20000, random_state=42)\n",
        "\n",
        "# 3. Combine them into one plotting table\n",
        "plot_df = pd.concat([normal_sample, anomalies_sample])"
      ],
      "metadata": {
        "id": "j4snywNHaW1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- STEP 7: THE ANOMALY MAP (FIXED LABELS) ---\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Map the -1 and 1 to actual words so the legend is automatic\n",
        "plot_df['Detection_Status'] = plot_df['ml_anomaly_flag'].map({1: 'Normal', -1: 'Anomaly'})\n",
        "\n",
        "sns.scatterplot(\n",
        "    data=plot_df,\n",
        "    x='my_custom_velocity',\n",
        "    y='amount',\n",
        "    hue='Detection_Status', # Use the new text column\n",
        "    palette={'Normal': 'lightgray', 'Anomaly': 'red'}, # Explicitly link color to name\n",
        "    alpha=0.6,\n",
        "    edgecolor=None\n",
        ")\n",
        "\n",
        "plt.yscale('log')\n",
        "plt.title('Fraud Detection: Transaction Amount vs. Velocity', fontsize=16)\n",
        "plt.xlabel('24-Hour Transaction Count (Velocity)', fontsize=12)\n",
        "plt.ylabel('Transaction Amount ($) - Log Scale', fontsize=12)\n",
        "plt.legend(title='AI Verdict') # Matplotlib will now generate this correctly\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "02UoBX0PPqo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the 'Red Dots' to a file\n",
        "ml_anomalies.to_csv('detected_fraud_anomalies.csv', index=False)\n",
        "print(\"File saved! Check the folder icon on the left to download it.\")"
      ],
      "metadata": {
        "id": "s6sG4A0vNXOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Business Impact Summary\n",
        "* **Efficiency:** Reduced the manual review workload by 98% (from 5M to 100k transactions).\n",
        "* **Speed:** The Isolation Forest model processes the entire dataset in seconds.\n",
        "* **Accuracy:** The system identifies both \"Big Hit\" fraud (high amount) and \"Smurfing\" (high frequency/velocity)."
      ],
      "metadata": {
        "id": "2wSWNEZiODoh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZFGvXmP7OGZN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}